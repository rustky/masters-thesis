{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc025cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507110d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. , 6.5, 7. ,\n",
       "       7.5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "exponents = np.arange(1,8,.5)\n",
    "exponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2abd3969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 32, 100, 316, 1000, 3162, 10000, 31622, 100000, 316228, 1000000, 3162278, 10000000, 31622776]\n"
     ]
    }
   ],
   "source": [
    "num_data_points = []\n",
    "data_list = []\n",
    "for i in range(len(exponents)):\n",
    "    num_data_points.append(math.ceil(10**exponents[i]))\n",
    "    if num_data_points[i] % 2 != 0:\n",
    "        num_data_points[i] = num_data_points[i] - 1\n",
    "    tensor = torch.from_numpy(np.random.standard_normal(num_data_points[i]))\n",
    "    data_list.append(tensor.type(torch.DoubleTensor))\n",
    "print(num_data_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f140ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "32\n",
      "100\n",
      "316\n",
      "1000\n",
      "3162\n",
      "10000\n",
      "31622\n",
      "100000\n",
      "316228\n",
      "1000000\n",
      "3162278\n",
      "10000000\n",
      "31622776\n"
     ]
    }
   ],
   "source": [
    "label_list = []\n",
    "for j in num_data_points:\n",
    "    temp = torch.from_numpy(np.repeat([0,1], j/2))\n",
    "    print(len(temp))\n",
    "    label_list.append(temp.type(torch.DoubleTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8872ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_exp = np.arange(-5,0.5,0.5)\n",
    "# time_exp\n",
    "# time_limits = []\n",
    "# for k in time_exp:\n",
    "#     time_limits.append(10**k)\n",
    "time_limit = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79312a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from square_loss import square_loss\n",
    "from squared_hinge_loss import squared_hinge_loss\n",
    "from naive_square_loss import naive_square_loss\n",
    "from naive_square_hinge_loss import naive_square_hinge_loss\n",
    "from torch.nn import BCEWithLogitsLoss as logisitic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5f16a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss = logisitic_loss()\n",
    "loss_dict ={\n",
    "#     \"naive_square\": naive_square_loss,\n",
    "    \"naive_square_hinge\": naive_square_hinge_loss,\n",
    "#     \"square_loss\":square_loss,\n",
    "    \"squared_hinge_loss\": squared_hinge_loss,\n",
    "#     \"logistic_loss\": log_loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d06a7aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38888e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squared_hinge_loss\n",
      "logistic_loss\n"
     ]
    }
   ],
   "source": [
    "times_dict_list = []\n",
    "for loss_name, loss_fun in loss_dict.items():\n",
    "    print(loss_name)\n",
    "    for x in range(len(data_list)):\n",
    "        time_vec = np.zeros(1)\n",
    "        for iter in range(1):\n",
    "            start_time = time.time()\n",
    "            if loss_name not in [\"logistic_loss\"]:\n",
    "                loss = loss_fun(data_list[x], label_list[x], 1)\n",
    "            else:\n",
    "                loss = loss_fun(data_list[x], label_list[x])\n",
    "                print(loss)\n",
    "            time_vec[iter] = time.time() - start_time\n",
    "        if (np.median(time_vec) < time_limit):\n",
    "            max_dict = {\n",
    "                \"loss_name\": loss_name,\n",
    "                \"lower_quantile\": np.quantile(time_vec, 0.25),\n",
    "                \"upper_quantile\": np.quantile(time_vec, 0.75),\n",
    "                \"median_time\": np.median(time_vec),\n",
    "                \"data_size\": num_data_points[x]\n",
    "            }\n",
    "            times_dict_list.append(pd.DataFrame(max_dict,index=[0]))\n",
    "        else:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2335f991",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_927212/1432658697.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes_dict_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndexes\u001b[0m \u001b[0mhave\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \"\"\"\n\u001b[0;32m--> 294\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "full_dt = pd.concat(times_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f509e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a2b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b0c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dt.to_csv(\"timing_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3a8d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
